# .gitHub/workflows/deploy-staging.yml
name: Build and Deploy to EKS

on:
  push:
    branches:
      - staging
  pull_request:
    branches:
      - staging

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: nodejs-app/nodejs-app
  EKS_CLUSTER_NAME: nodejs-app-cluster
  HELM_RELEASE_NAME: nodeapp
  HELM_CHART_PATH: ./helm/nodeapp-k8s-project

jobs:
  build-push-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build, tag, and push image to Amazon ECR
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Build the Docker image
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
          docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:latest
          
          # Push both tags to ECR
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
          
          echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.29.0'

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.13.0'

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      - name: Deploy to EKS using Helm
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Create namespace if it doesn't exist
          kubectl create namespace production --dry-run=client -o yaml | kubectl apply -f -
          
          # Deploy using Helm with updated image
          helm upgrade --install ${{ env.HELM_RELEASE_NAME }} ${{ env.HELM_CHART_PATH }} \
            --namespace production \
            --set image.repository=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \
            --set service.type=LoadBalancer \
            --set service.port=3000 \
            --set ingress.enabled=false \
            --set replicaCount=2 \
            --set autoscaling.enabled=true \
            --set autoscaling.minReplicas=2 \
            --set autoscaling.maxReplicas=4 \
            --set autoscaling.targetCPUUtilizationPercentage=70 \
            --wait \
            --timeout 5m

      - name: Get LoadBalancer URL
        run: |
          echo "Waiting for LoadBalancer to be ready..."
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=nodeapp-k8s-project -n production --timeout=300s
          
          # Get the LoadBalancer URL
          export LB_URL=$(kubectl get svc ${{ env.HELM_RELEASE_NAME }}-nodeapp-k8s-project -n production -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          
          echo "Application is accessible at: http://$LB_URL:3000"
          echo "LoadBalancer URL: $LB_URL" >> $GITHUB_STEP_SUMMARY
          
          # Wait for LoadBalancer to be fully ready
          sleep 30
          
          # Test the application
          curl -f http://$LB_URL:3000/health || echo "Health check endpoint not responding yet"

      - name: Verify Deployment
        run: |
          kubectl rollout status deployment/${{ env.HELM_RELEASE_NAME }}-nodeapp-k8s-project -n production
          kubectl get pods -n production
          kubectl get svc -n production